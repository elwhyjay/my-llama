# MY OWN LLAMA

- 한장의 gpu에서 돌아갈 모델을 가정함 (no parallelism)
- 7B 모델 기준

## llama 구현하기

- [x] rotary embedding 
- [x] GQA
- [x] rms-normalization 
- [x] SwiGLU
- [x] feedforward
- [x] attention
- [x] transformer block
- [x] tokenizer
- [ ] inference and generation
## TODO

### inference 관련 구현

- page attention 
- speculative decoding 
- kv cache 
